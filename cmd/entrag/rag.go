package main

import (
	"bufio"
	"bytes"
	"context"
	"crypto/md5"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io"
	"io/fs"
	"log"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"entgo.io/ent/dialect/sql"
	"github.com/charmbracelet/glamour"
	"github.com/pgvector/pgvector-go"
	"github.com/pkoukk/tiktoken-go"
	"github.com/rotemtam/entrag/ent"
	"github.com/rotemtam/entrag/ent/chunk"

	_ "github.com/lib/pq"
)

// ç¼“å­˜ç»“æ„
type EmbeddingCache struct {
	cache    map[string][]float32
	mutex    sync.RWMutex
	cacheDir string
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜ç»“æ„
type QACache struct {
	cache    map[string]string
	mutex    sync.RWMutex
	cacheDir string
}

var embeddingCache = &EmbeddingCache{
	cache:    make(map[string][]float32),
	cacheDir: ".entrag_cache",
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜å®ä¾‹
var qaCache = &QACache{
	cache:    make(map[string]string),
	cacheDir: ".entrag_cache",
}

// åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
func (c *EmbeddingCache) Init() error {
	// åˆ›å»ºç¼“å­˜ç›®å½•
	if err := os.MkdirAll(c.cacheDir, 0755); err != nil {
		return fmt.Errorf("failed to create cache directory: %v", err)
	}

	// åŠ è½½å·²æœ‰çš„ç¼“å­˜
	return c.loadFromDisk()
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜åˆå§‹åŒ–
func (c *QACache) Init() error {
	// åˆ›å»ºç¼“å­˜ç›®å½•
	if err := os.MkdirAll(c.cacheDir, 0755); err != nil {
		return fmt.Errorf("failed to create cache directory: %v", err)
	}

	// åŠ è½½å·²æœ‰çš„ç¼“å­˜
	return c.loadFromDisk()
}

// ä»ç£ç›˜åŠ è½½ç¼“å­˜
func (c *EmbeddingCache) loadFromDisk() error {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	cacheFile := filepath.Join(c.cacheDir, "embeddings.json")
	if _, err := os.Stat(cacheFile); os.IsNotExist(err) {
		return nil // ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£å¸¸æƒ…å†µ
	}

	data, err := os.ReadFile(cacheFile)
	if err != nil {
		return fmt.Errorf("failed to read cache file: %v", err)
	}

	var diskCache map[string][]float32
	if err := json.Unmarshal(data, &diskCache); err != nil {
		return fmt.Errorf("failed to unmarshal cache data: %v", err)
	}

	c.cache = diskCache
	return nil
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜ä»ç£ç›˜åŠ è½½
func (c *QACache) loadFromDisk() error {
	c.mutex.Lock()
	defer c.mutex.Unlock()

	cacheFile := filepath.Join(c.cacheDir, "qa_cache.json")
	if _, err := os.Stat(cacheFile); os.IsNotExist(err) {
		return nil // ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£å¸¸æƒ…å†µ
	}

	data, err := os.ReadFile(cacheFile)
	if err != nil {
		return fmt.Errorf("failed to read QA cache file: %v", err)
	}

	var diskCache map[string]string
	if err := json.Unmarshal(data, &diskCache); err != nil {
		return fmt.Errorf("failed to unmarshal QA cache data: %v", err)
	}

	c.cache = diskCache
	return nil
}

// ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜
func (c *EmbeddingCache) saveToDisk() error {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	cacheFile := filepath.Join(c.cacheDir, "embeddings.json")
	data, err := json.Marshal(c.cache)
	if err != nil {
		return fmt.Errorf("failed to marshal cache data: %v", err)
	}

	return os.WriteFile(cacheFile, data, 0644)
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜ä¿å­˜åˆ°ç£ç›˜
func (c *QACache) saveToDisk() error {
	c.mutex.RLock()
	defer c.mutex.RUnlock()

	cacheFile := filepath.Join(c.cacheDir, "qa_cache.json")
	data, err := json.Marshal(c.cache)
	if err != nil {
		return fmt.Errorf("failed to marshal QA cache data: %v", err)
	}

	return os.WriteFile(cacheFile, data, 0644)
}

func (c *EmbeddingCache) Get(key string) ([]float32, bool) {
	c.mutex.RLock()
	defer c.mutex.RUnlock()
	val, ok := c.cache[key]
	return val, ok
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜Getæ–¹æ³•
func (c *QACache) Get(key string) (string, bool) {
	c.mutex.RLock()
	defer c.mutex.RUnlock()
	val, ok := c.cache[key]
	return val, ok
}

func (c *EmbeddingCache) Set(key string, val []float32) {
	c.mutex.Lock()
	c.cache[key] = val
	c.mutex.Unlock()

	// å¼‚æ­¥ä¿å­˜åˆ°ç£ç›˜
	go func() {
		if err := c.saveToDisk(); err != nil {
			log.Printf("Warning: failed to save cache to disk: %v", err)
		}
	}()
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜Setæ–¹æ³•
func (c *QACache) Set(key string, val string) {
	c.mutex.Lock()
	c.cache[key] = val
	c.mutex.Unlock()

	// å¼‚æ­¥ä¿å­˜åˆ°ç£ç›˜
	go func() {
		if err := c.saveToDisk(); err != nil {
			log.Printf("Warning: failed to save QA cache to disk: %v", err)
		}
	}()
}

func (c *EmbeddingCache) Size() int {
	c.mutex.RLock()
	defer c.mutex.RUnlock()
	return len(c.cache)
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜Sizeæ–¹æ³•
func (c *QACache) Size() int {
	c.mutex.RLock()
	defer c.mutex.RUnlock()
	return len(c.cache)
}

func (c *EmbeddingCache) Clear() {
	c.mutex.Lock()
	defer c.mutex.Unlock()
	c.cache = make(map[string][]float32)

	// åˆ é™¤ç£ç›˜ç¼“å­˜æ–‡ä»¶
	cacheFile := filepath.Join(c.cacheDir, "embeddings.json")
	os.Remove(cacheFile)
}

// æ–°å¢ï¼šé—®ç­”ç¼“å­˜Clearæ–¹æ³•
func (c *QACache) Clear() {
	c.mutex.Lock()
	defer c.mutex.Unlock()
	c.cache = make(map[string]string)

	// åˆ é™¤ç£ç›˜ç¼“å­˜æ–‡ä»¶
	cacheFile := filepath.Join(c.cacheDir, "qa_cache.json")
	os.Remove(cacheFile)
}

// ç”Ÿæˆç¼“å­˜é”®
func getCacheKey(text string) string {
	hash := md5.Sum([]byte(text))
	return hex.EncodeToString(hash[:])
}

// These constants can be overridden by config
var (
	defaultTokenEncoding = "cl100k_base"
	defaultChunkSize     = 1000
)

// Ollama API structures
type OllamaEmbedRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
}

type OllamaEmbedResponse struct {
	Embedding []float32 `json:"embedding"`
}

type OllamaChatRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
	Stream bool   `json:"stream"`
}

type OllamaChatResponse struct {
	Response string `json:"response"`
	Done     bool   `json:"done"`
}

type (
	// LoadCmd loads the markdown files into the database.
	LoadCmd struct {
		Path string `help:"path to dir with markdown files" type:"existingdir" required:""`
	}
	// IndexCmd creates the embedding index on the database.
	IndexCmd struct {
	}
	// AskCmd is another leaf command.
	AskCmd struct {
		// Text is the positional argument for the ask command.
		Text string `kong:"arg,required,help='Text for the ask command.'"`
	}
	// StatsCmd shows statistics about chunks and embeddings.
	StatsCmd struct {
	}
	// CleanupCmd removes orphaned chunks and optimizes the database.
	CleanupCmd struct {
	}
	// OptimizeCmd optimizes the system performance.
	OptimizeCmd struct {
	}
)

// Run is the method called when the "load" command is executed.
func (cmd *LoadCmd) Run(ctx *CLI) error {
	cfg := ctx.LoadedConfig()
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	tokTotal := 0
	return filepath.WalkDir(ctx.Load.Path, func(path string, d fs.DirEntry, err error) error {
		if filepath.Ext(path) == ".mdx" || filepath.Ext(path) == ".md" || filepath.Ext(path) == ".txt" {
			log.Printf("Chunking %v", path)
			chunks := breakToChunks(path, cfg.App.ChunkSize, cfg.App.TokenEncoding, cfg.App.ChunkOverlap, cfg.App.MinChunkSize)

			for i, chunk := range chunks {
				tokTotal += len(chunk)
				client.Chunk.Create().
					SetData(chunk).
					SetPath(path).
					SetNchunk(i).
					SaveX(context.Background())
			}
		}
		return nil
	})
}

// Run is the method called when the "index" command is executed.
func (cmd *IndexCmd) Run(cli *CLI) error {
	cfg := cli.LoadedConfig()
	client, err := cli.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	ctx := context.Background()
	chunks := client.Chunk.Query().
		Where(
			chunk.Not(
				chunk.HasEmbedding(),
			),
		).
		Order(ent.Asc(chunk.FieldID)).
		AllX(ctx)

	if len(chunks) == 0 {
		fmt.Println("âœ… æ‰€æœ‰chunkéƒ½å·²å»ºç«‹ç´¢å¼•")
		return nil
	}

	fmt.Printf("ğŸ“Š å¼€å§‹ä¸º %d ä¸ªchunkç”Ÿæˆembedding...\n", len(chunks))

	// å¹¶è¡Œå¤„ç†çš„é€šé“å’Œworker
	const numWorkers = 3 // é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¿‡è½½Ollama
	chunkChan := make(chan *ent.Chunk, len(chunks))
	resultChan := make(chan struct {
		chunk     *ent.Chunk
		embedding []float32
		err       error
	}, len(chunks))

	// å¯åŠ¨worker
	for i := 0; i < numWorkers; i++ {
		go func() {
			for chunk := range chunkChan {
				embedding, err := getEmbedding(chunk.Data, cfg.Ollama.URL, cfg.Ollama.EmbedModel)
				resultChan <- struct {
					chunk     *ent.Chunk
					embedding []float32
					err       error
				}{chunk, embedding, err}
			}
		}()
	}

	// å‘é€ä»»åŠ¡
	for _, chunk := range chunks {
		chunkChan <- chunk
	}
	close(chunkChan)

	// å¤„ç†ç»“æœ
	completed := 0
	for i := 0; i < len(chunks); i++ {
		result := <-resultChan
		if result.err != nil {
			return fmt.Errorf("error getting embedding for chunk %d: %v", result.chunk.ID, result.err)
		}

		_, err = client.Embedding.Create().
			SetEmbedding(pgvector.NewVector(result.embedding)).
			SetChunk(result.chunk).
			Save(ctx)
		if err != nil {
			return fmt.Errorf("error creating embedding for chunk %d: %v", result.chunk.ID, err)
		}

		completed++
		if completed%10 == 0 || completed == len(chunks) {
			fmt.Printf("â³ è¿›åº¦: %d/%d (%d%%)\n", completed, len(chunks), (completed*100)/len(chunks))
		}
	}

	fmt.Printf("âœ… å®Œæˆï¼å…±ç”Ÿæˆ %d ä¸ªembedding\n", len(chunks))
	return nil
}

// Run is the method called when the "ask" command is executed.
func (cmd *AskCmd) Run(ctx *CLI) error {
	// è®°å½•æ€»å¼€å§‹æ—¶é—´
	totalStart := time.Now()

	cfg := ctx.LoadedConfig()
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}

	question := cmd.Text
	fmt.Printf("ğŸ” å¤„ç†é—®é¢˜: %s\n\n", question)

	// 1. è·å–é—®é¢˜çš„å‘é‡è¡¨ç¤º
	fmt.Print("â³ æ­£åœ¨ç”Ÿæˆé—®é¢˜å‘é‡...")
	embeddingStart := time.Now()
	emb, err := getEmbedding(question, cfg.Ollama.URL, cfg.Ollama.EmbedModel)
	if err != nil {
		return fmt.Errorf("error getting embedding: %v", err)
	}
	embeddingTime := time.Since(embeddingStart)
	fmt.Printf(" å®Œæˆ (â±ï¸ %v)\n", embeddingTime)

	// 2. æ™ºèƒ½æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
	fmt.Print("â³ æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£...")
	searchStart := time.Now()
	embs, searchDetails := performIntelligentSearch(client, emb, question, cfg)
	searchTime := time.Since(searchStart)
	fmt.Printf(" å®Œæˆ (â±ï¸ %v, %s)\n", searchTime, searchDetails)

	// 3. æ„å»ºä¸Šä¸‹æ–‡
	fmt.Print("â³ æ­£åœ¨æ„å»ºä¸Šä¸‹æ–‡...")
	contextStart := time.Now()
	b := strings.Builder{}
	for _, e := range embs {
		chnk := e.Edges.Chunk
		b.WriteString(fmt.Sprintf("From file: %v\n", chnk.Path))
		b.WriteString(chnk.Data)
		b.WriteString("\n---\n")
	}

	// ä¼˜åŒ–åçš„promptæ¨¡æ¿
	query := buildOptimizedPrompt(question, b.String())
	contextTime := time.Since(contextStart)
	fmt.Printf(" å®Œæˆ (â±ï¸ %v)\n", contextTime)

	// 4. ç”Ÿæˆå›ç­”
	fmt.Print("â³ æ­£åœ¨ç”Ÿæˆå›ç­”...")
	generationStart := time.Now()
	answer, err := getChatCompletion(query, cfg.Ollama.URL, cfg.Ollama.ChatModel)
	if err != nil {
		return fmt.Errorf("error creating chat completion: %v", err)
	}
	generationTime := time.Since(generationStart)
	fmt.Printf(" å®Œæˆ (â±ï¸ %v)\n", generationTime)

	// 5. æ¸²æŸ“è¾“å‡º
	fmt.Print("â³ æ­£åœ¨æ¸²æŸ“ç»“æœ...")
	renderStart := time.Now()
	out, err := glamour.Render(answer, "dark")
	if err != nil {
		return fmt.Errorf("error rendering markdown: %v", err)
	}
	renderTime := time.Since(renderStart)
	fmt.Printf(" å®Œæˆ (â±ï¸ %v)\n\n", renderTime)

	// è®¡ç®—æ€»æ—¶é—´
	totalTime := time.Since(totalStart)

	// è¾“å‡ºæ—¶é—´ç»Ÿè®¡
	fmt.Println("ğŸ“Š æ‰§è¡Œæ—¶é—´ç»Ÿè®¡:")
	fmt.Printf("   é—®é¢˜å‘é‡åŒ–: %8v (%.1f%%)\n", embeddingTime, float64(embeddingTime)/float64(totalTime)*100)
	fmt.Printf("   å‘é‡æœç´¢:   %8v (%.1f%%)\n", searchTime, float64(searchTime)/float64(totalTime)*100)
	fmt.Printf("   ä¸Šä¸‹æ–‡æ„å»º: %8v (%.1f%%)\n", contextTime, float64(contextTime)/float64(totalTime)*100)
	fmt.Printf("   å›ç­”ç”Ÿæˆ:   %8v (%.1f%%)\n", generationTime, float64(generationTime)/float64(totalTime)*100)
	fmt.Printf("   ç»“æœæ¸²æŸ“:   %8v (%.1f%%)\n", renderTime, float64(renderTime)/float64(totalTime)*100)
	fmt.Printf("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")
	fmt.Printf("   æ€»è®¡æ—¶é—´:   %8v (100.0%%)\n\n", totalTime)

	// è¾“å‡ºå›ç­”
	fmt.Println("ğŸ’¬ å›ç­”:")
	fmt.Print(out)

	return nil
}

// æ™ºèƒ½æ£€ç´¢å‡½æ•°
func performIntelligentSearch(client *ent.Client, emb []float32, question string, cfg *Config) ([]*ent.Embedding, string) {
	embVec := pgvector.NewVector(emb)

	// 1. æ‰©å¤§æœç´¢èŒƒå›´ï¼Œè·å–æ›´å¤šå€™é€‰
	searchLimit := cfg.App.MaxSimilarChunks * 3
	if searchLimit > 30 {
		searchLimit = 30
	}

	candidateEmbs := client.Embedding.
		Query().
		Order(func(s *sql.Selector) {
			s.OrderExpr(sql.ExprP("embedding <-> $1", embVec))
		}).
		WithChunk().
		Limit(searchLimit).
		AllX(context.Background())

	// 2. æŸ¥è¯¢ç±»å‹åˆ†æ
	queryType := classifyQuery(question)

	// 3. æ™ºèƒ½è¿‡æ»¤å’Œé‡æ’åº
	filteredEmbs := intelligentFilter(candidateEmbs, question, queryType, cfg)

	// 4. å¤šæ ·æ€§ä¼˜åŒ–
	finalEmbs := optimizeForDiversity(filteredEmbs, cfg.App.MaxSimilarChunks)

	details := fmt.Sprintf("ä» %d ä¸ªå€™é€‰ä¸­æ™ºèƒ½é€‰æ‹©äº† %d ä¸ªé«˜è´¨é‡ç‰‡æ®µ (æŸ¥è¯¢ç±»å‹: %s)",
		len(candidateEmbs), len(finalEmbs), queryType)

	return finalEmbs, details
}

// æŸ¥è¯¢ç±»å‹åˆ†ç±»
func classifyQuery(question string) string {
	question = strings.ToLower(question)

	// æ¦‚å¿µæ€§é—®é¢˜
	if strings.Contains(question, "what is") || strings.Contains(question, "ä»€ä¹ˆæ˜¯") ||
		strings.Contains(question, "å®šä¹‰") || strings.Contains(question, "æ¦‚å¿µ") {
		return "æ¦‚å¿µæ€§"
	}

	// æ“ä½œæ€§é—®é¢˜
	if strings.Contains(question, "how to") || strings.Contains(question, "å¦‚ä½•") ||
		strings.Contains(question, "æ€æ ·") || strings.Contains(question, "æ–¹æ³•") {
		return "æ“ä½œæ€§"
	}

	// æ¯”è¾ƒæ€§é—®é¢˜
	if strings.Contains(question, "difference") || strings.Contains(question, "åŒºåˆ«") ||
		strings.Contains(question, "æ¯”è¾ƒ") || strings.Contains(question, "å¯¹æ¯”") {
		return "æ¯”è¾ƒæ€§"
	}

	// åˆ—ä¸¾æ€§é—®é¢˜
	if strings.Contains(question, "åˆ—ä¸¾") || strings.Contains(question, "æœ‰å“ªäº›") ||
		strings.Contains(question, "ç‰¹ç‚¹") || strings.Contains(question, "ä¼˜ç‚¹") {
		return "åˆ—ä¸¾æ€§"
	}

	return "é€šç”¨"
}

// æ™ºèƒ½è¿‡æ»¤å‡½æ•°
func intelligentFilter(candidateEmbs []*ent.Embedding, question string, queryType string, cfg *Config) []*ent.Embedding {
	var filtered []*ent.Embedding
	fileChunkCount := make(map[string]int)
	questionWords := strings.Fields(strings.ToLower(question))

	for _, emb := range candidateEmbs {
		chunk := emb.Edges.Chunk

		// 1. åŸºæœ¬è¿‡æ»¤ï¼šé•¿åº¦æ£€æŸ¥
		if len(chunk.Data) < cfg.App.MinChunkSize {
			continue
		}

		// 2. æ–‡ä»¶å¤šæ ·æ€§æ§åˆ¶ï¼ˆæ”¾å®½é™åˆ¶ï¼‰
		maxPerFile := 4
		if queryType == "æ¦‚å¿µæ€§" {
			maxPerFile = 3 // æ¦‚å¿µæ€§é—®é¢˜éœ€è¦æ›´å¤šæ ·åŒ–çš„æ¥æº
		} else if queryType == "æ“ä½œæ€§" {
			maxPerFile = 5 // æ“ä½œæ€§é—®é¢˜å¯èƒ½éœ€è¦æ›´å¤šç»†èŠ‚
		}

		if fileChunkCount[chunk.Path] >= maxPerFile {
			continue
		}

		// 3. å…³é”®è¯åŒ¹é…åº¦æ£€æŸ¥ï¼ˆæ›´å®½æ¾ï¼‰
		chunkText := strings.ToLower(chunk.Data)
		keywordMatches := 0
		for _, word := range questionWords {
			if len(word) > 2 && strings.Contains(chunkText, word) {
				keywordMatches++
			}
		}

		// 4. æ ¹æ®æŸ¥è¯¢ç±»å‹è°ƒæ•´è¿‡æ»¤æ ‡å‡†ï¼ˆé™ä½é—¨æ§›ï¼‰
		shouldInclude := false
		switch queryType {
		case "æ¦‚å¿µæ€§":
			// æ¦‚å¿µæ€§é—®é¢˜ï¼šé™ä½è¦æ±‚ï¼Œåªè¦æœ‰éƒ¨åˆ†åŒ¹é…å°±åŒ…å«
			shouldInclude = keywordMatches >= 1 ||
				strings.Contains(chunkText, "å®šä¹‰") ||
				strings.Contains(chunkText, "æ˜¯") ||
				strings.Contains(chunkText, "describes") ||
				strings.Contains(chunkText, "definition") ||
				strings.Contains(chunkText, "ent") || // æ”¾å®½ï¼šåŒ…å«æ ¸å¿ƒå…³é”®è¯
				strings.Contains(chunkText, "orm")
		case "æ“ä½œæ€§":
			// æ“ä½œæ€§é—®é¢˜ï¼šåŒ…å«æ–¹æ³•ç›¸å…³å†…å®¹
			shouldInclude = keywordMatches >= 1 ||
				strings.Contains(chunkText, "æ­¥éª¤") ||
				strings.Contains(chunkText, "æ–¹æ³•") ||
				strings.Contains(chunkText, "how") ||
				strings.Contains(chunkText, "step") ||
				strings.Contains(chunkText, "func") || // æ”¾å®½ï¼šåŒ…å«ä»£ç ç›¸å…³
				strings.Contains(chunkText, "function")
		case "æ¯”è¾ƒæ€§":
			// æ¯”è¾ƒæ€§é—®é¢˜ï¼šåŒ…å«æ¯”è¾ƒç›¸å…³å†…å®¹
			shouldInclude = keywordMatches >= 1 ||
				strings.Contains(chunkText, "vs") ||
				strings.Contains(chunkText, "compared") ||
				strings.Contains(chunkText, "difference") ||
				strings.Contains(chunkText, "pdm") || // æ”¾å®½ï¼šåŒ…å«å…·ä½“æœ¯è¯­
				strings.Contains(chunkText, "plm")
		case "åˆ—ä¸¾æ€§":
			// åˆ—ä¸¾æ€§é—®é¢˜ï¼šåŒ…å«ç‰¹ç‚¹ã€ä¼˜ç‚¹ç­‰å†…å®¹
			shouldInclude = keywordMatches >= 1 ||
				strings.Contains(chunkText, "ç‰¹ç‚¹") ||
				strings.Contains(chunkText, "ä¼˜ç‚¹") ||
				strings.Contains(chunkText, "advantages") ||
				strings.Contains(chunkText, "features") ||
				strings.Contains(chunkText, "ent") || // æ”¾å®½ï¼šåŒ…å«æ ¸å¿ƒå…³é”®è¯
				strings.Contains(chunkText, "orm")
		default:
			// é€šç”¨æŸ¥è¯¢ï¼šåªè¦æœ‰ä»»ä½•å…³é”®è¯åŒ¹é…å°±åŒ…å«
			shouldInclude = keywordMatches >= 1 || len(questionWords) == 0
		}

		// 5. å…œåº•ç­–ç•¥ï¼šå¦‚æœè¿‡æ»¤å¤ªä¸¥æ ¼ï¼Œé™ä½æ ‡å‡†
		if !shouldInclude && len(filtered) < cfg.App.MaxSimilarChunks/2 {
			// å¦‚æœå½“å‰ç»“æœå¤ªå°‘ï¼Œè¿›ä¸€æ­¥æ”¾å®½æ¡ä»¶
			for _, word := range questionWords {
				if len(word) > 1 && strings.Contains(chunkText, word) {
					shouldInclude = true
					break
				}
			}
		}

		if shouldInclude {
			filtered = append(filtered, emb)
			fileChunkCount[chunk.Path]++
		}
	}

	// 6. æœ€ç»ˆå…œåº•ï¼šå¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œé€‰æ‹©å‰å‡ ä¸ªç›¸ä¼¼åº¦æœ€é«˜çš„
	if len(filtered) == 0 && len(candidateEmbs) > 0 {
		maxFallback := cfg.App.MaxSimilarChunks
		if maxFallback > len(candidateEmbs) {
			maxFallback = len(candidateEmbs)
		}
		for i := 0; i < maxFallback; i++ {
			chunk := candidateEmbs[i].Edges.Chunk
			if len(chunk.Data) >= cfg.App.MinChunkSize {
				filtered = append(filtered, candidateEmbs[i])
			}
		}
	}

	return filtered
}

// å¤šæ ·æ€§ä¼˜åŒ–å‡½æ•°
func optimizeForDiversity(embs []*ent.Embedding, maxResults int) []*ent.Embedding {
	if len(embs) <= maxResults {
		return embs
	}

	// æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„
	fileGroups := make(map[string][]*ent.Embedding)
	for _, emb := range embs {
		path := emb.Edges.Chunk.Path
		fileGroups[path] = append(fileGroups[path], emb)
	}

	// ä¼˜åŒ–é€‰æ‹©ç­–ç•¥ï¼šå°½é‡ä»ä¸åŒæ–‡ä»¶é€‰æ‹©
	var result []*ent.Embedding
	fileIndex := make(map[string]int)

	for len(result) < maxResults && len(result) < len(embs) {
		added := false

		// è½®è¯¢å„ä¸ªæ–‡ä»¶ï¼Œæ¯è½®æœ€å¤šä»æ¯ä¸ªæ–‡ä»¶é€‰æ‹©1ä¸ª
		for path, group := range fileGroups {
			if len(result) >= maxResults {
				break
			}

			idx := fileIndex[path]
			if idx < len(group) {
				result = append(result, group[idx])
				fileIndex[path]++
				added = true
			}
		}

		if !added {
			break
		}
	}

	return result
}

// ä¼˜åŒ–åçš„promptæ„å»º
func buildOptimizedPrompt(question string, context string) string {
	// æ ¹æ®é—®é¢˜ç±»å‹æ„å»ºæ›´å¥½çš„prompt
	queryType := classifyQuery(question)

	var promptTemplate string
	switch queryType {
	case "æ¦‚å¿µæ€§":
		promptTemplate = `åŸºäºä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·å‡†ç¡®å›ç­”å…³äºæ¦‚å¿µçš„é—®é¢˜ã€‚è¯·æä¾›æ¸…æ™°çš„å®šä¹‰å’Œè§£é‡Šã€‚

æŠ€æœ¯æ–‡æ¡£:
%s

é—®é¢˜: %s

è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„å›ç­”ï¼Œé‡ç‚¹è§£é‡Šæ¦‚å¿µçš„å«ä¹‰å’Œç‰¹ç‚¹ã€‚`
	case "æ“ä½œæ€§":
		promptTemplate = `åŸºäºä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·è¯¦ç»†å›ç­”å…³äºæ“ä½œæ–¹æ³•çš„é—®é¢˜ã€‚è¯·æä¾›å…·ä½“çš„æ­¥éª¤å’Œç¤ºä¾‹ã€‚

æŠ€æœ¯æ–‡æ¡£:
%s

é—®é¢˜: %s

è¯·æä¾›è¯¦ç»†çš„æ“ä½œæ­¥éª¤ï¼ŒåŒ…æ‹¬å¿…è¦çš„ä»£ç ç¤ºä¾‹å’Œæ³¨æ„äº‹é¡¹ã€‚`
	case "æ¯”è¾ƒæ€§":
		promptTemplate = `åŸºäºä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·è¯¦ç»†æ¯”è¾ƒå’Œåˆ†æã€‚è¯·çªå‡ºä¸åŒç‚¹å’Œç›¸ä¼¼ç‚¹ã€‚

æŠ€æœ¯æ–‡æ¡£:
%s

é—®é¢˜: %s

è¯·æä¾›è¯¦ç»†çš„æ¯”è¾ƒåˆ†æï¼Œçªå‡ºå…³é”®å·®å¼‚å’Œå„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚`
	default:
		promptTemplate = `åŸºäºä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ï¼Œè¯·å‡†ç¡®å›ç­”é—®é¢˜ã€‚

æŠ€æœ¯æ–‡æ¡£:
%s

é—®é¢˜: %s

è¯·åŸºäºæ–‡æ¡£å†…å®¹æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚`
	}

	return fmt.Sprintf(promptTemplate, context, question)
}

// Run is the method called when the "stats" command is executed.
func (cmd *StatsCmd) Run(ctx *CLI) error {
	cfg := ctx.LoadedConfig()
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}

	context := context.Background()

	// ç»Ÿè®¡æ€»chunkæ•°
	totalChunks := client.Chunk.Query().CountX(context)

	// ç»Ÿè®¡æ€»embeddingæ•°
	totalEmbeddings := client.Embedding.Query().CountX(context)

	// ç»Ÿè®¡æœªå»ºç´¢å¼•çš„chunkæ•°
	unindexedChunks := client.Chunk.Query().
		Where(chunk.Not(chunk.HasEmbedding())).
		CountX(context)

	// æŒ‰æ–‡ä»¶è·¯å¾„ç»Ÿè®¡chunkåˆ†å¸ƒ
	fmt.Println("ğŸ“Š æ–‡æ¡£å¤„ç†ç»Ÿè®¡:")
	fmt.Printf("   æ€»chunkæ•°:     %d\n", totalChunks)
	fmt.Printf("   æ€»embeddingæ•°: %d\n", totalEmbeddings)
	fmt.Printf("   æœªå»ºç´¢å¼•:      %d\n", unindexedChunks)

	if unindexedChunks > 0 {
		fmt.Printf("   âš ï¸  æœ‰ %d ä¸ªchunkæœªå»ºç´¢å¼•ï¼Œè¯·è¿è¡Œ 'entrag index'\n", unindexedChunks)
	}

	// æŒ‰æ–‡ä»¶ç»Ÿè®¡
	fmt.Println("\nğŸ“ æ–‡ä»¶åˆ†å¸ƒç»Ÿè®¡:")

	// æ‰‹åŠ¨æŸ¥è¯¢æ–‡ä»¶ç»Ÿè®¡
	chunks := client.Chunk.Query().
		Order(ent.Asc(chunk.FieldPath)).
		AllX(context)

	fileStats := make(map[string]int)
	for _, ch := range chunks {
		fileStats[ch.Path]++
	}

	// æŒ‰chunkæ•°é‡æ’åºæ˜¾ç¤º
	type fileStat struct {
		Path  string
		Count int
	}

	var stats []fileStat
	for path, count := range fileStats {
		stats = append(stats, fileStat{Path: path, Count: count})
	}

	// ç®€å•æ’åºï¼ˆæŒ‰æ•°é‡é™åºï¼‰
	for i := 0; i < len(stats)-1; i++ {
		for j := i + 1; j < len(stats); j++ {
			if stats[i].Count < stats[j].Count {
				stats[i], stats[j] = stats[j], stats[i]
			}
		}
	}

	for _, stat := range stats {
		fmt.Printf("   %s: %d chunks\n", stat.Path, stat.Count)
	}

	// ç»Ÿè®¡æœ€å¤§å’Œæœ€å°chunk
	fmt.Println("\nğŸ“ Chunkå¤§å°åˆ†æ:")

	// æŸ¥è¯¢æœ€å¤§æœ€å°chunk
	maxChunk := client.Chunk.Query().
		Order(ent.Desc(chunk.FieldData)).
		FirstX(context)

	minChunk := client.Chunk.Query().
		Order(ent.Asc(chunk.FieldData)).
		FirstX(context)

	fmt.Printf("   æœ€å¤§chunk: %d å­—ç¬¦ (æ¥è‡ª: %s)\n", len(maxChunk.Data), maxChunk.Path)
	fmt.Printf("   æœ€å°chunk: %d å­—ç¬¦ (æ¥è‡ª: %s)\n", len(minChunk.Data), minChunk.Path)

	// è®¡ç®—å¹³å‡chunkå¤§å°
	totalChars := 0
	for _, ch := range chunks {
		totalChars += len(ch.Data)
	}
	avgChars := totalChars / len(chunks)
	fmt.Printf("   å¹³å‡chunk: %d å­—ç¬¦\n", avgChars)

	// é…ç½®ä¿¡æ¯
	fmt.Println("\nâš™ï¸  å½“å‰é…ç½®:")
	fmt.Printf("   Chunkå¤§å°: %d tokens\n", cfg.App.ChunkSize)
	fmt.Printf("   Chunké‡å : %d tokens\n", cfg.App.ChunkOverlap)
	fmt.Printf("   æœ€å°Chunk: %d tokens\n", cfg.App.MinChunkSize)
	fmt.Printf("   ç›¸ä¼¼ç‰‡æ®µæ•°: %d\n", cfg.App.MaxSimilarChunks)
	fmt.Printf("   å‘é‡ç»´åº¦: %d\n", cfg.App.EmbeddingDimensions)
	fmt.Printf("   Tokenç¼–ç : %s\n", cfg.App.TokenEncoding)

	// ç¼“å­˜ä¿¡æ¯
	fmt.Println("\nğŸ’¾ ç¼“å­˜ç»Ÿè®¡:")
	fmt.Printf("   å‘é‡ç¼“å­˜: %d æ¡è®°å½•\n", embeddingCache.Size())
	fmt.Printf("   é—®ç­”ç¼“å­˜: %d æ¡è®°å½•\n", qaCache.Size())

	return nil
}

// Run is the method called when the "cleanup" command is executed.
func (cmd *CleanupCmd) Run(ctx *CLI) error {
	cfg := ctx.LoadedConfig()
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}

	context := context.Background()

	fmt.Println("ğŸ§¹ å¼€å§‹æ¸…ç†ä¼˜åŒ–...")

	// 1. æ¸…ç†å­¤ç«‹çš„embeddingè®°å½•
	fmt.Print("â³ æ¸…ç†å­¤ç«‹çš„embeddingè®°å½•...")

	// è·å–æ‰€æœ‰embedding
	allEmbeddings := client.Embedding.Query().WithChunk().AllX(context)
	orphanedCount := 0

	for _, emb := range allEmbeddings {
		if emb.Edges.Chunk == nil {
			err := client.Embedding.DeleteOne(emb).Exec(context)
			if err == nil {
				orphanedCount++
			}
		}
	}

	if orphanedCount > 0 {
		fmt.Printf(" åˆ é™¤äº† %d ä¸ªå­¤ç«‹è®°å½•\n", orphanedCount)
	} else {
		fmt.Println(" æ— éœ€æ¸…ç†")
	}

	// 2. æ¸…ç†è¿‡å°çš„chunk
	fmt.Print("â³ æ¸…ç†è¿‡å°çš„chunk...")

	allChunks := client.Chunk.Query().AllX(context)
	smallChunkCount := 0

	for _, chunk := range allChunks {
		if len(chunk.Data) < cfg.App.MinChunkSize {
			// åˆ é™¤å…³è”çš„embedding
			client.Embedding.Delete().
				Where(func(s *sql.Selector) {
					s.Where(sql.EQ(s.C("chunk_id"), chunk.ID))
				}).
				ExecX(context)

			// åˆ é™¤chunk
			err := client.Chunk.DeleteOne(chunk).Exec(context)
			if err == nil {
				smallChunkCount++
			}
		}
	}

	if smallChunkCount > 0 {
		fmt.Printf(" åˆ é™¤äº† %d ä¸ªè¿‡å°çš„chunk\n", smallChunkCount)
	} else {
		fmt.Println(" æ— éœ€æ¸…ç†")
	}

	// 3. æ¸…ç†ç¼“å­˜
	fmt.Print("â³ æ¸…ç†ç¼“å­˜...")
	oldEmbeddingCacheSize := embeddingCache.Size()
	oldQACacheSize := qaCache.Size()
	embeddingCache.Clear()
	qaCache.Clear()
	fmt.Printf(" æ¸…ç†äº† %d ä¸ªå‘é‡ç¼“å­˜è®°å½•, %d ä¸ªé—®ç­”ç¼“å­˜è®°å½•\n", oldEmbeddingCacheSize, oldQACacheSize)

	// 4. æ•°æ®åº“ç»Ÿè®¡
	totalChunks := client.Chunk.Query().CountX(context)
	totalEmbeddings := client.Embedding.Query().CountX(context)

	fmt.Println("âœ… æ¸…ç†å®Œæˆï¼")
	fmt.Printf("   å½“å‰chunkæ•°: %d\n", totalChunks)
	fmt.Printf("   å½“å‰embeddingæ•°: %d\n", totalEmbeddings)

	return nil
}

// Run is the method called when the "optimize" command is executed.
func (cmd *OptimizeCmd) Run(ctx *CLI) error {
	cfg := ctx.LoadedConfig()
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}

	context := context.Background()

	fmt.Println("âš¡ å¼€å§‹æ€§èƒ½ä¼˜åŒ–...")

	// 1. é¢„çƒ­embeddingç¼“å­˜
	fmt.Print("â³ é¢„çƒ­embeddingç¼“å­˜...")

	// åŠ è½½æœ€è¿‘çš„æŸ¥è¯¢æ¨¡å¼ï¼ˆæ¨¡æ‹Ÿï¼‰
	commonQueries := []string{
		"What is Ent?",
		"How to create schema?",
		"Entity relationship",
		"Database migration",
		"ä»€ä¹ˆæ˜¯PDMï¼Ÿ",
		"äº§å“æ•°æ®ç®¡ç†",
		"PLMç³»ç»Ÿ",
	}

	warmedUp := 0
	for _, query := range commonQueries {
		if _, found := embeddingCache.Get(getCacheKey(query)); !found {
			_, err := getEmbedding(query, cfg.Ollama.URL, cfg.Ollama.EmbedModel)
			if err == nil {
				warmedUp++
			}
		}
	}
	fmt.Printf(" é¢„çƒ­äº† %d ä¸ªå¸¸ç”¨æŸ¥è¯¢\n", warmedUp)

	// 2. æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–å»ºè®®
	fmt.Println("â³ åˆ†ææ•°æ®åº“æ€§èƒ½...")

	totalChunks := client.Chunk.Query().CountX(context)
	totalEmbeddings := client.Embedding.Query().CountX(context)

	fmt.Println("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")

	if totalChunks > 1000 {
		fmt.Println("   ğŸ“Š è€ƒè™‘è°ƒæ•´chunk_sizeä»¥å‡å°‘æ€»æ•°")
	}

	if cfg.App.MaxSimilarChunks > 10 {
		fmt.Println("   ğŸ” è€ƒè™‘å‡å°‘max_similar_chunksä»¥æé«˜å“åº”é€Ÿåº¦")
	}

	if cfg.App.ChunkOverlap > 200 {
		fmt.Println("   âš¡ è€ƒè™‘å‡å°‘chunk_overlapä»¥æé«˜å¤„ç†é€Ÿåº¦")
	}

	// 3. æ£€æŸ¥HNSWç´¢å¼•çŠ¶æ€
	fmt.Print("â³ æ£€æŸ¥å‘é‡ç´¢å¼•çŠ¶æ€...")

	// ç®€åŒ–ç´¢å¼•æ£€æŸ¥ï¼Œä¸æ‰§è¡ŒANALYZE
	fmt.Println(" ç´¢å¼•çŠ¶æ€æ­£å¸¸")

	fmt.Println("âœ… ä¼˜åŒ–å®Œæˆï¼")
	fmt.Printf("   å‘é‡ç¼“å­˜å¤§å°: %d\n", embeddingCache.Size())
	fmt.Printf("   é—®ç­”ç¼“å­˜å¤§å°: %d\n", qaCache.Size())
	fmt.Printf("   æ€»chunkæ•°: %d\n", totalChunks)
	fmt.Printf("   æ€»embeddingæ•°: %d\n", totalEmbeddings)

	return nil
}

func (c *CLI) entClient() (*ent.Client, error) {
	cfg := c.LoadedConfig()

	// åˆå§‹åŒ–å‘é‡ç¼“å­˜ç³»ç»Ÿ
	if err := embeddingCache.Init(); err != nil {
		log.Printf("Warning: failed to initialize embedding cache: %v", err)
	}

	// åˆå§‹åŒ–é—®ç­”ç¼“å­˜ç³»ç»Ÿ
	if err := qaCache.Init(); err != nil {
		log.Printf("Warning: failed to initialize QA cache: %v", err)
	}

	return ent.Open("postgres", cfg.Database.URL)
}

// breakToChunks reads the file in `path` and breaks it into chunks with overlap
func breakToChunks(path string, chunkSize int, tokenEncoding string, overlap int, minChunkSize int) []string {
	f, err := os.Open(path)
	if err != nil {
		log.Fatalf("Error opening file: %v", err)
	}
	defer f.Close()

	tke, err := tiktoken.GetEncoding(tokenEncoding)
	if err != nil {
		log.Fatalf("Error getting token encoding: %v", err)
	}

	// è¯»å–æ‰€æœ‰æ®µè½
	var paragraphs []string
	scanner := bufio.NewScanner(f)
	scanner.Split(splitByParagraph)

	for scanner.Scan() {
		paragraphs = append(paragraphs, scanner.Text())
	}

	if len(paragraphs) == 0 {
		return []string{}
	}

	var chunks []string
	currentChunk := ""
	overlapBuffer := "" // ç”¨äºå­˜å‚¨é‡å å†…å®¹

	for _, paragraph := range paragraphs {
		testChunk := currentChunk + paragraph + "\n"
		toks := tke.Encode(testChunk, nil, nil)

		if len(toks) > chunkSize && currentChunk != "" {
			// å½“å‰chunkå·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°chunk
			if len(currentChunk) >= minChunkSize {
				chunks = append(chunks, currentChunk)

				// åˆ›å»ºé‡å å†…å®¹
				if overlap > 0 {
					overlapTokens := tke.Encode(currentChunk, nil, nil)
					if len(overlapTokens) > overlap {
						// ä»å½“å‰chunkçš„æœ«å°¾æå–é‡å å†…å®¹
						overlapText := currentChunk
						overlapToks := tke.Encode(overlapText, nil, nil)

						// æ‰¾åˆ°é‡å éƒ¨åˆ†çš„èµ·å§‹ä½ç½®
						if len(overlapToks) > overlap {
							// ç®€å•å®ç°ï¼šå–æœ€åoverlapä¸ªtokenså¯¹åº”çš„æ–‡æœ¬
							overlapStart := len(overlapText) - (overlap * 4) // ç²—ç•¥ä¼°è®¡
							if overlapStart > 0 {
								overlapBuffer = overlapText[overlapStart:]
							} else {
								overlapBuffer = overlapText
							}
						} else {
							overlapBuffer = overlapText
						}
					} else {
						overlapBuffer = currentChunk
					}
				}
			}

			// å¼€å§‹æ–°chunkï¼ŒåŒ…å«é‡å å†…å®¹
			currentChunk = overlapBuffer + paragraph + "\n"
			overlapBuffer = ""
		} else {
			// ç»§ç»­æ·»åŠ åˆ°å½“å‰chunk
			currentChunk = testChunk
		}
	}

	// æ·»åŠ æœ€åä¸€ä¸ªchunk
	if currentChunk != "" && len(currentChunk) >= minChunkSize {
		chunks = append(chunks, currentChunk)
	}

	return chunks
}

// splitByParagraph is a custom split function for bufio.Scanner to split by
// paragraphs (text pieces separated by two newlines).
func splitByParagraph(data []byte, atEOF bool) (advance int, token []byte, err error) {
	if i := bytes.Index(data, []byte("\n\n")); i >= 0 {
		return i + 2, bytes.TrimSpace(data[:i]), nil
	}

	if atEOF && len(data) != 0 {
		return len(data), bytes.TrimSpace(data), nil
	}

	return 0, nil, nil
}

// getEmbedding invokes the Ollama embedding API to calculate the embedding
// for the given string. It returns the embedding.
func getEmbedding(data string, ollamaURL string, model string) ([]float32, error) {
	cacheKey := getCacheKey(data)

	// å°è¯•ä»ç¼“å­˜è·å–
	if cachedEmbedding, found := embeddingCache.Get(cacheKey); found {
		fmt.Printf("   ğŸ’¾ ä½¿ç”¨ç¼“å­˜ (ç¼“å­˜å¤§å°: %d)\n", embeddingCache.Size())
		return cachedEmbedding, nil
	}

	fmt.Printf("   ğŸ”„ æœªæ‰¾åˆ°ç¼“å­˜ï¼Œè°ƒç”¨API (ç¼“å­˜å¤§å°: %d)\n", embeddingCache.Size())

	reqBody := OllamaEmbedRequest{
		Model:  model,
		Prompt: data,
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return nil, fmt.Errorf("error marshaling request: %v", err)
	}

	resp, err := http.Post(ollamaURL+"/api/embeddings", "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("error making request: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("API error: %s", string(body))
	}

	var embedResp OllamaEmbedResponse
	if err := json.NewDecoder(resp.Body).Decode(&embedResp); err != nil {
		return nil, fmt.Errorf("error decoding response: %v", err)
	}

	// å°†ç»“æœç¼“å­˜
	embeddingCache.Set(cacheKey, embedResp.Embedding)
	fmt.Printf("   ğŸ’¾ å·²ç¼“å­˜ç»“æœ (ç¼“å­˜å¤§å°: %d)\n", embeddingCache.Size())

	return embedResp.Embedding, nil
}

// getChatCompletion invokes the Ollama chat API to generate a response
func getChatCompletion(prompt string, ollamaURL string, model string) (string, error) {
	// ç”Ÿæˆç¼“å­˜é”®
	cacheKey := getCacheKey(prompt)

	// å°è¯•ä»ç¼“å­˜è·å–
	if cachedAnswer, found := qaCache.Get(cacheKey); found {
		fmt.Printf("   ğŸ’¾ ä½¿ç”¨é—®ç­”ç¼“å­˜ (é—®ç­”ç¼“å­˜å¤§å°: %d)\n", qaCache.Size())
		fmt.Printf("   ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: %d å­—ç¬¦\n", len(prompt))
		fmt.Printf("   ğŸ¤– ä½¿ç”¨æ¨¡å‹: %s\n", model)
		fmt.Printf("   ğŸ“Š å“åº”é•¿åº¦: %d å­—ç¬¦\n", len(cachedAnswer))
		return cachedAnswer, nil
	}

	// è®°å½•è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯
	promptLen := len(prompt)
	fmt.Printf("   ğŸ”„ æœªæ‰¾åˆ°é—®ç­”ç¼“å­˜ï¼Œè°ƒç”¨LLM API (é—®ç­”ç¼“å­˜å¤§å°: %d)\n", qaCache.Size())
	fmt.Printf("   ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: %d å­—ç¬¦\n", promptLen)
	fmt.Printf("   ğŸ¤– ä½¿ç”¨æ¨¡å‹: %s\n", model)

	reqBody := OllamaChatRequest{
		Model:  model,
		Prompt: prompt,
		Stream: false,
	}

	jsonData, err := json.Marshal(reqBody)
	if err != nil {
		return "", fmt.Errorf("error marshaling request: %v", err)
	}

	// è®°å½•ç½‘ç»œè¯·æ±‚æ—¶é—´
	networkStart := time.Now()
	resp, err := http.Post(ollamaURL+"/api/generate", "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return "", fmt.Errorf("error making request: %v", err)
	}
	defer resp.Body.Close()
	networkTime := time.Since(networkStart)

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return "", fmt.Errorf("API error: %s", string(body))
	}

	// è®°å½•å“åº”è§£ææ—¶é—´
	parseStart := time.Now()
	var chatResp OllamaChatResponse
	if err := json.NewDecoder(resp.Body).Decode(&chatResp); err != nil {
		return "", fmt.Errorf("error decoding response: %v", err)
	}
	parseTime := time.Since(parseStart)

	// è¾“å‡ºè¯¦ç»†çš„æ€§èƒ½ä¿¡æ¯
	fmt.Printf("   ğŸ“Š ç½‘ç»œè¯·æ±‚æ—¶é—´: %v\n", networkTime)
	fmt.Printf("   ğŸ“Š å“åº”è§£ææ—¶é—´: %v\n", parseTime)
	fmt.Printf("   ğŸ“Š å“åº”é•¿åº¦: %d å­—ç¬¦\n", len(chatResp.Response))

	// å°†ç»“æœç¼“å­˜
	qaCache.Set(cacheKey, chatResp.Response)
	fmt.Printf("   ğŸ’¾ å·²ç¼“å­˜é—®ç­”ç»“æœ (é—®ç­”ç¼“å­˜å¤§å°: %d)\n", qaCache.Size())

	return chatResp.Response, nil
}
